name: Performance issue
description: Report performance problems or optimization opportunities in omni-moe kernels
title: "[PERFORMANCE] "
labels:
  - performance
assignees:
  - LoserCheems
  - SNHuan
  - wubingheng111
body:
  - type: markdown
    attributes:
      value: |
        Provide enough detail about performance regressions or optimization opportunities so we can reproduce and diagnose them.
  - type: textarea
    id: issue-description
    attributes:
      label: Performance issue description
      description: Summarise the performance problem.
      placeholder: Kernel runs slower than expected on certain devices or sizes...
    validations:
      required: true
  - type: textarea
    id: current-performance
    attributes:
      label: Current performance metrics
      description: Share benchmark numbers and configuration (problem size n/m/k, device, dtype, backend, throughput, memory usage).
      placeholder: |
        Kernel: omni router / omni sparse ffn ...
        tokens: 4096
        num_keys: 64
        top_k: 8
        Device: cuda:0 (or mps / cpu)
        Dtype: float16
        Backend: pytorch / triton
        Speed: 0.12 ms/iteration
        Memory: 1.2 GB
    validations:
      required: true
  - type: textarea
    id: expected-performance
    attributes:
      label: Expected performance
      description: Explain what performance you expect and the baseline you are comparing against.
      placeholder: Expect similar or better speed than PyTorch or another framework on the same hardware...
  - type: textarea
    id: environment
    attributes:
      label: Environment information
      description: Run the following command and paste the output.
      placeholder: |
        python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA: {torch.version.cuda}'); print(f'GPU: {torch.cuda.get_device_name() if torch.cuda.is_available() else \"None\"}')"
      render: shell
    validations:
      required: true
  - type: textarea
    id: benchmark-code
    attributes:
      label: Benchmark code
      description: Provide the code snippet or script used to measure performance.
      render: python
  - type: textarea
    id: profiling
    attributes:
      label: Profiling information
      description: Include relevant excerpts from nsys, nvprof, or PyTorch profiler if available.
  - type: textarea
    id: system-info
    attributes:
      label: System information
      description: GPU model, compute capability, CPU, RAM, and other hardware details.
      placeholder: RTX 4090 24GB, compute capability 8.9, Intel i9-14900K, 64GB RAM
  - type: textarea
    id: additional-context
    attributes:
      label: Additional context
      description: Mention regressions, different batch sizes, attention patterns, or other observations.
